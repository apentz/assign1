---
title: "Matrix Factorisation Method"
author: "Vignette Author"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Matrix Factorisation Method}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
# load packages
library(cf),
library(dplyr),
library(tidyr)
)
```

### Objectives

Collaborative Filtering using Matrix Factorisation is 1 of 3 techniques used to build a Recommender System in the cf package. A demonstration of how to use the cf package for this purpose follows. 

The demonstration uses a book ratings dataset of 10,000 users that have rated 150 books on a scale from 0 to 10 and is loaded with the package. 

The package can be used in this way on any similar dataset.

The objectives of this vignette are as follows:  
\n- To transform the data into the format required  
\n- To split the data into a training and testing (validation) set  
\n- To train a few models and evaulate their performance on the training and testing (validation) set  
\n  - experimenting with a few different number of latent factors (k)  
\n  - including adding L2 regularisation  
\n  - including adding Bias  
\n  - including adding L2 regularisation and Bias  
\n- To select the best model based on the out of sample (test) RMSE  
\n- To make predictions using the best model  
\n- To make predictions for a new reader  


### 1. Transform the data

This is a repeat of the EDA and User Based CF vignettes so will not be explained.

```{r}

# load packages
library(cf)
library(dplyr)
library(tidyr)

# 1. remove zeros
book_ratings_with_names <- book_ratings %>% 
  left_join(book_info) %>% 
  filter(Book.Rating != 0) %>%
  select(User.ID, Book.Title, Book.Rating)
head(book_ratings_with_names)

# 2. remove duplicates
duplicates <- book_ratings_with_names[c("1009","16885"),]
duplicates
book_ratings_with_names_deduped <- book_ratings_with_names %>% 
  anti_join(duplicates)
head(book_ratings_with_names_deduped)

```


### 2. Split the data into a training and testing (validation) set

We will select 80% for training and 20% for testing (validation of out of sample predictions).

```{r}

set.seed(123)
# create an index
index_long <- sample(seq_len(nrow(book_ratings_with_names_deduped)), 
                size = round(nrow(book_ratings_with_names_deduped)*0.8,0))

# split the data
train_long <- book_ratings_with_names_deduped[index_long,]    # train the model on 80% of the data
test_long <- book_ratings_with_names_deduped[-index_long,]    # hold out 20%
blanks <- test_long                   # create a copy of the test ratings
blanks[,3] <- NA                      # remove the test ratings and replace with NAs (blanks)
train <- rbind(train_long,blanks)     # combine NAs with the train set to preserve matrix size

# convert to wide matrix format
# convert training set from long to wide matrix and fill with NAs
train_wide <- cf::long_to_wide(train, NA)
train_wide <- cf::convert_to_matrix(train_wide) 
# convert testing set from long to wide matrix and fill with NAs
test_wide <- cf::long_to_wide(test_long, NA)      
test_wide <- cf::convert_to_matrix(test_wide)

# let's take a look at ratings for a for a selection of readers and books
demo <- train_wide[c("23872","63360","98783","16795", "11676"),
     c("1st to die: a novel","angela's ashes", "black and blue",
       "fahrenheit 451", "hannibal", "silence of the lambs",
       "interview with the vampire", "stones from the river", 
       "the fellowship of the ring (the lord of the rings, part 1)",
       "the horse whisperer","the vampire lestat (vampire chronicles, book ii)")]
# let's transpose it to make it easier to display
t(demo)

dim(train_wide)  # 7503
dim(test_wide)   # 2687
  
```


### 3. Train a few models and evaulate their performance on the training and testing (validation) set

The NNLM:nnmf function is useful to quickly build models, evaluate errors and make predictions. The `mf_ ...` collection of functions therefore build on top of this by setting some of the parameters we are interested in such as `method` = 'scd', `loss` = 'mse', `check.k` = FALSE. 

The idea is to be able to quickly iterate between building the model with the `mf_model_build` function, assess the accuracy with the `mf_model_rmse` function and either rebuild or predict using the `mf_predict` function.


#### 3.1. A simple model, without L2 regularisation or Bias

We will start with a simple model, without L2 regularisation and without Bias, and experiment with a few values of k, the number of latent factors. Latent factors summarise characteristics in the data that pertain to readers and books. For example, readers latent factors may be related to their demographics (age, culture, reading proficiency, etc.) and books latent factors may be related to a genre (classical, thriller, science fiction, etc.).

We are interested in observing the effect on the test RMSE as k changes. The model was trained with 80% of the data and the test RMSE represents the error on unseen observations. A relatively low RMSE and high test RMSE indicates overfitting (high variance, low bias) as the model performs well on the given data but less well on out of sample data. Ideally, we should pick the model that minimises the RMSE on the training set because we need to cater for new readers as well as existing readers.


```{r}

set.seed(123)

rmse_model <- data.frame(0, nrow = 4, ncol = 3)

for (i in 1:4){ 
  
model <- cf::mf_model_build(train_wide, k_latent = i, 
                         alpha = 0, beta = 0, 
                         bias = "FALSE")

rmse_model[i,] <- cf::mf_model_rmse(model, train_wide, test_wide)
colnames(rmse_model) <- c("rmse_train", "rmse_test", "rmse_all")
  
}

rmse_model

```


We can see that as k increases the RMSE on the training set decreases however RMSE on the test set increases or becomes unmeasurable. k=1 seems like the best choice so far (lowest RMSE test).

This is likely to be because there are 3631 readers that have only rated 1 book so it is difficult for the model to learn a pattern from 1 observation. 


#### 3.1. Adding L2 regularisation

Now we will add L2 regularisation by setting `alpha` and `beta` in the `mf_model_build` function. We will try low values of `alpha` and `beta`, high values of `alpha` and `beta`. The arbitrary numbers we have picked for are: low of 0.0000001 (1e-07), middle of 1, high of 5. Again, we are interested in observing the effect on the training RMSE and lower is better.


```{r}

set.seed(123)

rmse_model <- data.frame(0, nrow = 5, ncol = 3)

for (i in 1:5){ 
  
model <- cf::mf_model_build(train_wide, k_latent = 1, 
                         alpha = i, beta = i, 
                         bias = "FALSE")

rmse_model[i,] <- cf::mf_model_rmse(model, train_wide, test_wide)
colnames(rmse_model) <- c("rmse_train", "rmse_test", "rmse_all")
  
}

rmse_model

```


We can see that as `alpha` and `beta` increase, the RMSE on the training set mostly increases however RMSE on the test set mostly decreases. `alpha` = `beta` = 1 seems like the best choice (lowest RMSE test) but the RMSE test is still higher than the simpler model so there is no benefit in adding L2 Regularisation .

We set k = 1 but now let's set `alpha` = `beta` = 1 and vary k.

```{r}

set.seed(123)

rmse_model <- data.frame(0, nrow = 8, ncol = 3)

for (i in 1:8){ 
  
model <- cf::mf_model_build(train_wide, k_latent = i, 
                         alpha = 1, beta = 1, 
                         bias = "FALSE")

rmse_model[i,] <- cf::mf_model_rmse(model, train_wide, test_wide)
colnames(rmse_model) <- c("rmse_train", "rmse_test", "rmse_all")
  
}

rmse_model

```

No luck. k=1 still gives the lowest test RMSE but we have seen better results earlier on.


#### 3.2. Adding Bias

Bias is used to downweight the effect of someone who only rates high or only rates low so that they contribute less to the overall scores and have less influence on the predictions. We will experiement without L2, with Bias and for a few values of k.

```{r}

set.seed(123)

rmse_model <- data.frame(0, nrow = 6, ncol = 3)

for (i in 1:3){ 
  
model <- cf::mf_model_build(train_wide, k_latent = i, 
                         alpha = 0, beta = 0, 
                         bias = "TRUE")           
# the function is designed to make it easy to add Bias

rmse_model[i,] <- cf::mf_model_rmse(model, train_wide, test_wide)
colnames(rmse_model) <- c("rmse_train", "rmse_test", "rmse_all")
  
}

rmse_model


```

k=1 gives the lowest test RMSE but we have seen better results earlier on.


#### 3.2. Adding L2 regularisation and Bias

Let's add L2 regularisation and bias at the same time:

```{r}

set.seed(123)

rmse_model <- data.frame(0, nrow = 8, ncol = 3)

for (i in 1:8){ 
  
model <- cf::mf_model_build(train_wide, k_latent = 1, 
                         alpha = i, beta = i, 
                         bias = "TRUE")           
# the function is designed to make it easy to add Bias

rmse_model[i,] <- cf::mf_model_rmse(model, train_wide, test_wide)
colnames(rmse_model) <- c("rmse_train", "rmse_test", "rmse_all")
  
}

rmse_model

```

We see an improvement for `alpha` = `beta` = 6. Now let's set `alpha` = `beta` = 6 and vary k. 

```{r}

set.seed(123)

rmse_model <- data.frame(0, nrow = 5, ncol = 3)

for (i in 1:5){ 
  
model <- cf::mf_model_build(train_wide, k_latent = i, 
                         alpha = 6, beta = 6, 
                         bias = "TRUE")           
# the function is designed to make it easy to add Bias

rmse_model[i,] <- cf::mf_model_rmse(model, train_wide, test_wide)
colnames(rmse_model) <- c("rmse_train", "rmse_test", "rmse_all")
  
}

rmse_model

```

It doesn't seem like we are able to get a lower test RMSE than the very first simple model with no L2 regularisation and no Bias (1.052). We will now use that model to make predictions.


### 4. Model Selection

```{r}

# save the final model so that we can use it to make predictions
model_final <- cf::mf_model_build(train_wide, k_latent = 1, 
                         alpha = 0, beta = 0, 
                         bias = "FALSE")

```


### 5. Model Predictions

```{r}

# make predictions
predictions <- cf::mf_predict(model_final)

# here's a recap of the ratings of a selection of users we looked at earlier
t(demo)

# let's compare that to the model predicitons
t(predictions[c("23872","63360","98783","16795", "11676"),
     c("1st to die: a novel","angela's ashes", "black and blue",
       "fahrenheit 451", "hannibal", "silence of the lambs",
       "interview with the vampire", "stones from the river", 
       "the fellowship of the ring (the lord of the rings, part 1)",
       "the horse whisperer","the vampire lestat (vampire chronicles, book ii)")])

```

Some are close such as reader `23872` who rated "fahrenheit 451" at 9 and the prediction is 8.78. Some are not so close such as reader `11676` who rated "the horse whisperer" 1 and the prediction is 7.48.


### 6. Recommendations based on Model Predictions

```{r}

user <- c("23872","63360","98783","16795", "11676")
used <- book_ratings_with_names_deduped[user,]

ratings_wide <- cf::long_to_wide(book_ratings_with_names_deduped, NA)
ratings_wide <- cf::convert_to_matrix(ratings_wide)

# for 1 user
mf_recommend(user = "23872", predictions, ratings_wide, predict_n = 10)

# for all the other demo users:
lapply(c("63360","98783","16795", "11676"), mf_recommend, 
       predictions, ratings_wide, predict_n = 10)

```


### 7. Recommendations for a new user

```{r}

# create data for new users
users_new <- data.frame(User.ID = (rep(max(book_ratings$User.ID)+1,3)), 
                        ISBN = c("0440234743", "0971880107", "0345417623"), 
                        Book.Rating = c(2, 5, 3))

# remove ISBN and add Book.Title
users_new <- users_new %>% 
  left_join(book_info, by = "ISBN") %>% 
  select(User.ID,Book.Title,Book.Rating)





```



